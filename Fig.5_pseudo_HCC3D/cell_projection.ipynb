{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Packages\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from tifffile import imread, imwrite\n",
    "\n",
    "from pprint import pprint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\n",
    "    \"pgf.texsystem\": \"xelatex\",      # 使用 XeLaTeX，如果不需要 LaTeX 公式渲染，可以省略\n",
    "    'font.family': 'serif',          # 字体设置为衬线字体\n",
    "    'text.usetex': False,            # 禁用 LaTeX，使用 Matplotlib 内置文字渲染\n",
    "    'pgf.rcfonts': False,            # 禁用 pgf 的默认字体管理\n",
    "    'pdf.fonttype': 42,              # 确保字体为 TrueType 格式，可被 Illustrator 编辑\n",
    "    'ps.fonttype': 42,               # EPS 文件也使用 TrueType 格式\n",
    "    'figure.dpi': 300,               # 设置图形分辨率\n",
    "    'savefig.dpi': 300,              # 保存的图形文件分辨率\n",
    "    'axes.unicode_minus': False,     # 避免负号问题\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workdir \n",
    "BASE_DIR = Path(r'G:\\spatial_data')\n",
    "\n",
    "# data dir\n",
    "processed_dir = BASE_DIR / 'processed'\n",
    "\n",
    "# analysis dir\n",
    "CRT_PJT = '20250222_combined_analysis_of_pseudo_HCC3D'\n",
    "analysis_dir = BASE_DIR / 'analysis' / CRT_PJT\n",
    "typ_path = analysis_dir / \"cell_typing\"\n",
    "proj_path = analysis_dir / \"projection\"\n",
    "proj_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['RUN_ID_LIST', 'image_shape'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(analysis_dir / \"project_params.yaml\", \"r\") as file:\n",
    "    project_params = yaml.safe_load(file)\n",
    "project_params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaris merge channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tifffile\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "def create_ome_xml(channels, size_x, size_y, size_z=1, size_t=1, dtype='uint16'):\n",
    "    \"\"\"\n",
    "    创建符合 OME 标准的 XML 元数据。\n",
    "    \"\"\"\n",
    "    OME_NAMESPACE = \"http://www.openmicroscopy.org/Schemas/OME/2016-06\"\n",
    "    ET.register_namespace('ome', OME_NAMESPACE)\n",
    "    \n",
    "    ome = ET.Element(\"{%s}OME\" % OME_NAMESPACE, xmlns=OME_NAMESPACE)\n",
    "    image = ET.SubElement(ome, \"Image\", ID=\"Image:0\")\n",
    "    pixels = ET.SubElement(image, \"Pixels\",\n",
    "                           DimensionOrder=\"XYZCT\",\n",
    "                           Type=dtype,  # 根据实际数据类型调整\n",
    "                           SizeX=str(size_x),\n",
    "                           SizeY=str(size_y),\n",
    "                           SizeZ=str(size_z),\n",
    "                           SizeC=str(len(channels)),\n",
    "                           SizeT=str(size_t),\n",
    "                           BigEndian=\"false\",\n",
    "                           SignificantBits=str(np.dtype(dtype).itemsize * 8))\n",
    "    \n",
    "    for i, channel in enumerate(channels):\n",
    "        ET.SubElement(pixels, \"Channel\",\n",
    "                      ID=f\"Channel:0:{i}\",\n",
    "                      Name=channel.get('name', f'Channel_{i+1}'),\n",
    "                      SamplesPerPixel=\"1\")\n",
    "    \n",
    "    tree = ET.ElementTree(ome)\n",
    "    return ET.tostring(ome, encoding='utf-8', method='xml').decode('utf-8')\n",
    "\n",
    "def ImarisFileConverter(chn_files, output_path, temp_merge_dir=None, tiles=None, \n",
    "                        imaris_convert_path=r\"C:\\Program Files\\Bitplane\\ImarisFileConverter 9.6.0\\ImarisConvert.exe\", ):\n",
    "    \"\"\"\n",
    "    处理所有TIFF 文件为OME-TIFF文件，并使用ImarisConvert.exe转换为.ims文件。\n",
    "    \"\"\"\n",
    "    dir_name = os.path.dirname(output_path)\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    if temp_merge_dir is None:\n",
    "        temp_merge_dir = os.path.join(dir_name, \"temp_merge\")\n",
    "    os.makedirs(temp_merge_dir, exist_ok=True)\n",
    "            \n",
    "    # 读取每个通道的 TIFF 文件\n",
    "    channel_images = []\n",
    "    for filepath in chn_files:\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Missing file: {filepath}.\")\n",
    "        else:\n",
    "            img = tifffile.imread(filepath)\n",
    "            # img[img > 65000] = 0  # 修复异常像素值\n",
    "            channel_images.append(img)\n",
    "\n",
    "    # 获取每个通道的形状并确保一致\n",
    "    shapes = [img.shape for img in channel_images]\n",
    "    # print(f\"image shapes: {shapes}\")\n",
    "    \n",
    "    if len(set(shapes)) != 1:\n",
    "        raise ValueError(f\"Image shapes do not match: {shapes}.\")\n",
    "    \n",
    "    shape = shapes[0]\n",
    "    if len(shape) == 2:\n",
    "        size_y, size_x = shape\n",
    "        size_z = 1\n",
    "        size_t = 1\n",
    "        combined_image = np.stack(channel_images, axis=0)  # (C, Y, X)\n",
    "        axes = 'CYX'\n",
    "    elif len(shape) == 3:\n",
    "        size_z, size_y, size_x = shape\n",
    "        size_t = 1\n",
    "        combined_image = np.stack(channel_images, axis=0)  # (C, Z, Y, X)\n",
    "        axes = 'CZYX'\n",
    "    elif len(shape) == 4:\n",
    "        size_t, size_z, size_y, size_x = shape\n",
    "        combined_image = np.stack(channel_images, axis=0)  # (C, T, Z, Y, X)\n",
    "        axes = 'CTZYX'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image dimensions: {shape}.\")\n",
    "        \n",
    "    # 确定数据类型\n",
    "    dtype = str(combined_image.dtype)\n",
    "    \n",
    "    # 创建 OME-XML 元数据\n",
    "    try:\n",
    "        ome_xml = create_ome_xml(\n",
    "            channels=[{'name': ch} for ch in chn_files],\n",
    "            size_x=size_x,\n",
    "            size_y=size_y,\n",
    "            size_z=size_z,\n",
    "            size_t=size_t,\n",
    "            dtype=dtype\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating OME-XML: {e}. Skipping.\")\n",
    "    \n",
    "    # 保存为 OME-TIFF\n",
    "    combined_tif_path = os.path.join(temp_merge_dir, f\"combined.ome.tif\")\n",
    "    try:\n",
    "        tifffile.imwrite(\n",
    "            combined_tif_path,\n",
    "            combined_image,\n",
    "            photometric='minisblack',\n",
    "            metadata={'axes': axes, 'ome': ome_xml},\n",
    "            ome=True\n",
    "        )\n",
    "        # print(f\"Successfully wrote OME-TIFF at {combined_tif_path}\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error writing OME-TIFF: {e}.\")\n",
    "    \n",
    "    # 转换为 Imaris 文件\n",
    "    cmd = [\n",
    "        str(imaris_convert_path),\n",
    "        '-if', 'OmeTiff',\n",
    "        '-i', str(combined_tif_path),\n",
    "        '-of', 'Imaris5',\n",
    "        '-o', str(output_path),\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # print(f\"Running ImarisConvert.exe...\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            pass\n",
    "            # print(f\"Successfully converted to {output_path}\")\n",
    "        else:\n",
    "            print(f\"Error converting:\")\n",
    "            print(result.stdout)\n",
    "            print(result.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running ImarisConvert.exe: {e}\")\n",
    "    \n",
    "    # 清理临时 OME-TIFF 文件\n",
    "    if os.path.exists(combined_tif_path): os.remove(combined_tif_path)\n",
    "\n",
    "    # 清理临时合并文件夹\n",
    "    if os.path.exists(temp_merge_dir): shutil.rmtree(temp_merge_dir)\n",
    "    # print(\"All cycles have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def color_cells_uint16(tif_shape, convex_dict, df, key, resize=(1, 1)):\n",
    "    \"\"\"\n",
    "    生成uint16类型的TIFF图像\n",
    "    \n",
    "    参数：\n",
    "    tif_shape : tuple\n",
    "        目标图像形状，格式为(height, width)\n",
    "    convex_dict : dict\n",
    "        细胞编号到凸包顶点坐标的字典，顶点为Nx2数组，每行是(y, x)\n",
    "    df : pandas.DataFrame\n",
    "        包含细胞灰度值的数据框，需有'gray'列\n",
    "    \n",
    "    返回：\n",
    "    numpy.ndarray (dtype=uint16)\n",
    "    \"\"\"\n",
    "    # 初始化uint16图像\n",
    "    tif_image = np.zeros(tif_shape, dtype=np.uint16)\n",
    "    \n",
    "    # 提取灰度值并转换为uint16\n",
    "    gray_dict = df[key].astype(np.uint16).to_dict()\n",
    "    \n",
    "    for cell, gray_value in gray_dict.items():\n",
    "        if cell not in convex_dict: continue\n",
    "        # 获取细胞的凸包顶点\n",
    "        vertices = convex_dict[cell]\n",
    "        # 转换为OpenCV所需格式 (N,1,2)\n",
    "        # print(0, vertices)\n",
    "        pts = vertices[:, ::-1]\n",
    "        # print(1, pts)\n",
    "        pts = pts.reshape((-1, 1, 2))\n",
    "        pts = pts * list(resize)\n",
    "        pts = pts.astype(np.int32)\n",
    "        # print(2, pts)\n",
    "        cv2.fillPoly(tif_image, [pts], color=int(gray_value))\n",
    "    return tif_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create hulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import ConvexHull\n",
    "\n",
    "# def create_hull(rna_labeled):\n",
    "#     hulls = {}\n",
    "#     df_group = rna_labeled.groupby(\"Cell Index\")\n",
    "#     for group in tqdm(df_group, desc=\"hull\"):\n",
    "#         coordinates = group[1][[\"Y\", \"X\"]].values\n",
    "#         try: hull = ConvexHull(coordinates)\n",
    "#         except: continue\n",
    "#         coordinate_path = np.vstack((coordinates[hull.vertices, 0], coordinates[hull.vertices, 1])).T\n",
    "#         hulls[group[0]] = coordinate_path\n",
    "#     return hulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hulls_all = dict()\n",
    "# for RUN_ID in project_params['RUN_ID_LIST']:\n",
    "#     slice = RUN_ID.split(\"_\")[-1]\n",
    "#     rna_labeled = pd.read_csv(processed_dir / RUN_ID / 'segmented' / \"rna_labeled.csv\")\n",
    "#     hulls_slice = create_hull(rna_labeled)\n",
    "#     # hulls all is hulls_slice with each key add an f'-{slice}'\n",
    "#     hulls_all.update({f\"{key}-{slice}\": value for key, value in hulls_slice.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(proj_path/ \"cell_hulls.pkl\", \"wb\") as f: pickle.dump(hulls_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_proj_path = proj_path / 'celltype'\n",
    "cell_type_proj_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_path/ \"cell_hulls.pkl\", \"rb\") as f: hulls_all = pickle.load(f)\n",
    "adata = sc.read_h5ad(typ_path / 'combine_adata_st.h5ad')\n",
    "adata = adata[adata.obs['type']!='other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(analysis_dir, 'cell_typing_params.yaml'), 'r') as f:\n",
    "    annotaiton_params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "print(annotaiton_params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = 0.125\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "key = 'leiden_type'\n",
    "save_dir = cell_type_proj_path / f\"{key}\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# draw only the region of interest\n",
    "stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "for slice in range(1, 21):\n",
    "    slice_df = adata.obs.loc[adata.obs['slice'] == slice, [key]].astype(np.uint16) + 1\n",
    "    stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(0.125, 0.125))\n",
    "imwrite(save_dir / f\"{key}.tif\", stack_image, metadata={'axes': 'ZXY'})\n",
    "\n",
    "# save differnet gray scale in different channels\n",
    "for gray_scale in np.unique(stack_image):\n",
    "    if gray_scale == 0: continue\n",
    "    mask = stack_image == gray_scale\n",
    "    mask = mask.astype(np.uint16) * gray_scale\n",
    "    imwrite(save_dir / f\"{key}_{gray_scale}.tif\", mask, metadata={'axes': 'ZXY'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "key = 'leiden_type'\n",
    "save_dir = cell_type_proj_path / f\"{key}\"\n",
    "chn_files = sorted([str(save_dir/f) for f in os.listdir(save_dir) if re.search(r'_\\d+\\.tif$', f) and os.path.isfile(os.path.join(save_dir, f))],\n",
    "    key=lambda x: int(re.search(r'_(\\d+)\\.tif$', x).group(1)))\n",
    "ImarisFileConverter(chn_files, output_path=save_dir/'combined.ims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leiden_subtyp列e和subtype列是一一对应的，打印leiden_subtyp列e对应的subtype列的值\n",
    "for i in sorted([int(_) for _ in adata.obs['leiden_type'].unique()]):\n",
    "    name = list(adata.obs.loc[adata.obs['leiden_type'] == str(i), 'type'].unique())[0]\n",
    "    print(f\"\\'{name}\\'\", end=',')\n",
    "    # print(f\"{i+1}\\t{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = 0.125\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "key = 'leiden_subtype'\n",
    "save_dir = cell_type_proj_path / f\"{key}\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# draw only the region of interest\n",
    "stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "for slice in range(1, 21):\n",
    "    slice_df = adata.obs.loc[adata.obs['slice'] == slice, [key]].astype(np.uint16) + 1\n",
    "    stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(0.125, 0.125))\n",
    "imwrite(save_dir / f\"{key}.tif\", stack_image, metadata={'axes': 'ZXY'})\n",
    "\n",
    "# save differnet gray scale in different channels\n",
    "for gray_scale in np.unique(stack_image):\n",
    "    if gray_scale == 0: continue\n",
    "    mask = stack_image == gray_scale\n",
    "    mask = mask.astype(np.uint16) * gray_scale\n",
    "    imwrite(save_dir / f\"{key}_{gray_scale}.tif\", mask, metadata={'axes': 'ZXY'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'leiden_subtype'\n",
    "save_dir = cell_type_proj_path / f\"{key}\"\n",
    "chn_files = sorted([str(save_dir/f) for f in os.listdir(save_dir) if re.search(r'_\\d+\\.tif$', f) and os.path.isfile(os.path.join(save_dir, f))],\n",
    "    key=lambda x: int(re.search(r'_(\\d+)\\.tif$', x).group(1)))\n",
    "ImarisFileConverter(chn_files, output_path=save_dir/'combined.ims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leiden_subtyp列e和subtype列是一一对应的，打印leiden_subtyp列e对应的subtype列的值\n",
    "for i in sorted([int(_) for _ in adata.obs['leiden_subtype'].unique()]):\n",
    "    name = list(adata.obs.loc[adata.obs['leiden_subtype'] == str(i), 'subtype'].unique())[0]\n",
    "    # print(f\"{i+1}\\t{name}\")\n",
    "    print(f\"\\'{name}\\'\", end=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gene expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = proj_path / \"gene_expression\"\n",
    "output_path.mkdir(exist_ok=True)\n",
    "with open(proj_path/ \"cell_hulls.pkl\", \"rb\") as f: hulls_all = pickle.load(f)\n",
    "adata = sc.read_h5ad(typ_path / 'adata.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HBV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  3.  5.  9. 11. 17. 29.]\n"
     ]
    }
   ],
   "source": [
    "gene = 'HBV'\n",
    "obs_ksy = f'{gene}_level'\n",
    "gene_exp = adata.raw[:, gene].X\n",
    "# separate gene to 1-8 by percentile\n",
    "percentile_bins = np.percentile(gene_exp, [70, 90, 95, 99, 99.5, 99.9, 99.99])\n",
    "print(percentile_bins)\n",
    "\n",
    "gene_exp = gene_exp.flatten()\n",
    "mapped_levels = np.digitize(gene_exp, bins=percentile_bins, right=True) + 1\n",
    "adata.obs[obs_ksy] = mapped_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CD4+',\n",
       " 'other',\n",
       " 'Mait',\n",
       " 'Monocyte',\n",
       " 'Macrophage',\n",
       " 'DC',\n",
       " 'T_reg',\n",
       " 'Neutrophil',\n",
       " 'Tumor',\n",
       " 'NK',\n",
       " 'CD8+',\n",
       " 'B',\n",
       " 'Endo',\n",
       " 'Ep',\n",
       " 'Mast',\n",
       " 'Liver',\n",
       " 'CAF']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs.type.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = 0.125\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "key = obs_ksy\n",
    "save_dir = output_path / f\"{key}\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "adata_use = adata[adata.obs.type.isin(['Tumor', 'Liver', 'Ep', 'Endo', 'CAF'])].copy()\n",
    "# draw only the region of interest\n",
    "stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "for slice in range(1, 21):\n",
    "    slice_df = adata_use.obs.loc[adata_use.obs['slice'] == slice, [key]].astype(np.uint16)\n",
    "    stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(downsample, downsample))\n",
    "imwrite(save_dir / f\"{key}.tif\", stack_image, metadata={'axes': 'ZXY'})\n",
    "\n",
    "# save differnet gray scale in different channels\n",
    "for gray_scale in np.unique(stack_image):\n",
    "    if gray_scale == 0: continue\n",
    "    mask = stack_image == gray_scale\n",
    "    mask = mask.astype(np.uint16) * gray_scale\n",
    "    imwrite(save_dir / f\"{key}_{gray_scale}.tif\", mask, metadata={'axes': 'ZXY'})\n",
    "\n",
    "# combine all channels\n",
    "chn_files = sorted([str(save_dir/f) for f in os.listdir(save_dir) if re.search(r'_\\d+\\.tif$', f) and os.path.isfile(os.path.join(save_dir, f))],\n",
    "    key=lambda x: int(re.search(r'_(\\d+)\\.tif$', x).group(1)))\n",
    "ImarisFileConverter(chn_files, output_path=output_path/f'{key}.ims')\n",
    "shutil.rmtree(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_tumor_plot_AFP_subsample = sc.pp.subsample(adata[adata.obs.subtype == 'Tumor_AFP+'],\n",
    "                                                 n_obs=45730,\n",
    "                                                 copy=True)\n",
    "adata_tumor_plot_GPC3 = adata[adata.obs.subtype == 'Tumor_GPC3+']\n",
    "adata_tumor_plot = adata[adata.obs.index.isin(list(adata_tumor_plot_AFP_subsample.obs.index)+list(adata_tumor_plot_GPC3.obs.index))]\n",
    "adata_tumor_plot.write(r'E:\\TMC\\cell_typing\\results\\2023.9.28-_PRISM_HCC_final_downstream_analysis\\2023.10.6_AFPsubsample\\adata_AFPsubsampled_and_GPC3+.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "before_filter = Counter(list(adata_tumor_plot_AFP_subsample.obs['layer']))\n",
    "after_filter = Counter(list(adata_tumor_plot_GPC3.obs['layer']))\n",
    "\n",
    "data = dict(sorted(before_filter.items(), key=lambda d: int(d[0].replace('layer','')), reverse=False))\n",
    "courses = list(data.keys())\n",
    "values = list(data.values())\n",
    "fig = plt.figure(figsize = (15, 3))\n",
    "# creating the bar plot\n",
    "plt.bar(courses, values)\n",
    "\n",
    "data = dict(sorted(after_filter.items(), key=lambda d: int(d[0].replace('layer','')), reverse=False))\n",
    "courses = list(data.keys())\n",
    "values = list(data.values())\n",
    "fig = plt.figure(figsize = (15, 3))\n",
    "# creating the bar plot\n",
    "plt.bar(courses, values)\n",
    "\n",
    "courses = list(data.keys())\n",
    "values = [list(dict(sorted(before_filter.items(), key=lambda d: int(d[0].replace('layer','')), reverse=False)).values())[_]/list(dict(sorted(after_filter.items(), key=lambda d: int(d[0].replace('layer','')), reverse=False)).values())[_] for _ in range(len(list(dict(sorted(after_filter.items(), key=lambda d: int(d[0].replace('layer','')), reverse=False)).values())))]\n",
    "fig = plt.figure(figsize = (15, 3))\n",
    "# creating the bar plot\n",
    "plt.bar(courses, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['AFP_content'] = [0]*len(adata)\n",
    "\n",
    "for slice in range(20):\n",
    "    tmp1 = adata[adata.obs.layer == f'layer{slice}']\n",
    "    tmp1_index = tmp1.obs.index\n",
    "    tmp1.obs.index = [_.split('-')[0] for _ in tmp1.obs.index]\n",
    "\n",
    "    tmp2 = adata[adata.obs.layer == f'layer{slice}']\n",
    "    tmp2.obs.index = [_.split('-')[0] for _ in tmp2.obs.index]\n",
    "\n",
    "    tmp2 = tmp2[tmp1.obs.index]\n",
    "\n",
    "    adata.obs['AFP_content'][tmp1_index] = pd.Series([_[0] for _ in tmp2[:,'AFP'].X],index=tmp1_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(ncols=1,nrows=1,figsize=(20,5))\n",
    "# a=adata[adata.obs.AFP_content!=0].obs['AFP_content']\n",
    "# sns.histplot(a, bins=100, stat='count', alpha=1, kde=True,\n",
    "#             edgecolor='white', linewidth=0.5,\n",
    "#             # log=True,\n",
    "#             ax=ax,\n",
    "#             line_kws=dict(color='black', alpha=0.7, linewidth=1.5, label='KDE'),\n",
    "#             # binrange=[0,100]\n",
    "#             )\n",
    "# y=ax.get_lines()[0].get_ydata()\n",
    "# maxima = [float(j/len(y)*(max(a)-min(a))+min(a)) for j in argrelextrema(-np.array(y), np.less)[0]]\n",
    "\n",
    "# for submaxima in maxima:\n",
    "#     ax.axvline(x=submaxima, color='r', alpha=0.5, linestyle='--')\n",
    "    \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = adata[adata.obs.AFP_content != 0]\n",
    "# tmp = tmp[tmp.obs.tmp_leiden != '-2']\n",
    "content = sorted(list(tmp.obs.AFP_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(i+1, content.index(i+1)/len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HBV_grade = {\n",
    "    0: 1,\n",
    "    1: 2,\n",
    "    2: 3,\n",
    "    3: 4,\n",
    "    4: 5,\n",
    "    5: 6,\n",
    "    6: 7,\n",
    "    7: 9,\n",
    "    8: 11,\n",
    "    9: 15,\n",
    "    10: 20,\n",
    "}\n",
    "\n",
    "# HBV_grade = sorted(HBV_grade.items(), key=lambda x: -x[1])\n",
    "\n",
    "# combine_adata_st.obs.leiden['HBV_grade'] = pd.Categorical([0]*len(combine_adata_st), categories=list(ROI_mask.keys()) + ['other'], ordered=False)\n",
    "adata.obs[\"AFP_grade\"] = [-2] * len(adata)\n",
    "for cell in tqdm(adata.obs.index):\n",
    "    for grade, value in HBV_grade.items():\n",
    "        if adata.obs['AFP_content'].loc[cell] >= value:\n",
    "            adata.obs['AFP_grade'].loc[cell] = grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write(r'e:\\TMC\\cell_typing\\results\\2023.9.28-_PRISM_HCC_final_downstream_analysis\\2023.10.12_AFP_content\\adata_AFP_content.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detailed percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = adata[adata.obs.HBV_content != 0]\n",
    "# tmp = tmp[tmp.obs.tmp_leiden != '-2']\n",
    "content = list(tmp.obs.HBV_content)\n",
    "content = sorted(content)\n",
    "# np.percentile(content, [34,57,74,83,85,90,93,96,97,98,98.5,99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(i+1, content.index(i+1)/len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HBV_grade = {\n",
    "    0: 1,\n",
    "    1: 2,\n",
    "    2: 3,\n",
    "    3: 4,\n",
    "    4: 5,\n",
    "    5: 6,\n",
    "    6: 7,\n",
    "    7: 8,\n",
    "    8: 11,\n",
    "}\n",
    "\n",
    "# HBV_grade = sorted(HBV_grade.items(), key=lambda x: -x[1])\n",
    "\n",
    "# combine_adata_st.obs.leiden['HBV_grade'] = pd.Categorical([0]*len(combine_adata_st), categories=list(ROI_mask.keys()) + ['other'], ordered=False)\n",
    "adata.obs[\"HBV_grade_detailed\"] = [-2] * len(adata)\n",
    "for cell in tqdm(adata.obs.index):\n",
    "    for grade, value in HBV_grade.items():\n",
    "        if adata.obs['HBV_content'].loc[cell] >= value:\n",
    "            adata.obs['HBV_grade_detailed'].loc[cell] = grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGATE projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def color_cells_uint16(tif_shape, convex_dict, df, key, resize=(1, 1)):\n",
    "    \"\"\"\n",
    "    生成uint16类型的TIFF图像\n",
    "    \n",
    "    参数：\n",
    "    tif_shape : tuple\n",
    "        目标图像形状，格式为(height, width)\n",
    "    convex_dict : dict\n",
    "        细胞编号到凸包顶点坐标的字典，顶点为Nx2数组，每行是(y, x)\n",
    "    df : pandas.DataFrame\n",
    "        包含细胞灰度值的数据框，需有'gray'列\n",
    "    \n",
    "    返回：\n",
    "    numpy.ndarray (dtype=uint16)\n",
    "    \"\"\"\n",
    "    # 初始化uint16图像\n",
    "    tif_image = np.zeros(tif_shape, dtype=np.uint16)\n",
    "    \n",
    "    # 提取灰度值并转换为uint16\n",
    "    gray_dict = df[key].astype(np.uint16).to_dict()\n",
    "    \n",
    "    for cell, gray_value in gray_dict.items():\n",
    "        if cell not in convex_dict: continue\n",
    "        # 获取细胞的凸包顶点\n",
    "        vertices = convex_dict[cell]\n",
    "        # 转换为OpenCV所需格式 (N,1,2)\n",
    "        # print(0, vertices)\n",
    "        pts = vertices[:, ::-1]\n",
    "        # print(1, pts)\n",
    "        pts = pts.reshape((-1, 1, 2))\n",
    "        pts = pts * list(resize)\n",
    "        pts = pts.astype(np.int32)\n",
    "        # print(2, pts)\n",
    "        cv2.fillPoly(tif_image, [pts], color=int(gray_value))\n",
    "    return tif_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGATE_path = analysis_dir / \"STAGATE\"\n",
    "output_path = proj_path / \"STAGATE\"\n",
    "output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(proj_path/ \"cell_hulls.pkl\", \"rb\") as f: hulls_all = pickle.load(f)\n",
    "adata_STAGATE = sc.read_h5ad(STAGATE_path / 'adata_STAGATE.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mclust_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save column key as different gray scale\n",
    "from tifffile import imwrite\n",
    "\n",
    "downsample = 0.25\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "key = 'mclust_11'\n",
    "stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "for slice in range(1, 21):\n",
    "    slice_df = adata_STAGATE.obs.loc[adata_STAGATE.obs['slice'] == slice, [key]]\n",
    "    stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(downsample, downsample))\n",
    "imwrite(output_path / f\"{key}_downsample.tif\", stack_image, metadata={'axes': 'ZXY'})\n",
    "\n",
    "# save different gray scale in different channels\n",
    "from tifffile import imread\n",
    "\n",
    "stack_image = imread(output_path / f\"{key}_downsample.tif\")\n",
    "print(stack_image.shape)\n",
    "for gray_scale in np.unique(stack_image):\n",
    "    if gray_scale == 0: continue\n",
    "    mask = stack_image == gray_scale\n",
    "    mask = mask.astype(np.uint16) * gray_scale\n",
    "    imwrite(output_path / f\"{key}_{gray_scale}_downsample.tif\", mask, metadata={'axes': 'ZXY'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tumor_AFP+': [1],\n",
      " 'Tumor_GPC3+': [3],\n",
      " 'Tumor_pro_CXCL13': [4],\n",
      " 'Tumor_Liver': [10],\n",
      " 'Liver': [11],\n",
      " 'CAF': [5, 9],\n",
      " 'Epthelial_Neutrophil': [2, 6, 7],\n",
      " 'B': [8]}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(analysis_dir / \"STAGATE_analysis_params.yaml\"), \"r\") as f:\n",
    "    STAGATE_params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "STAGATE_annotate = STAGATE_params['STAGATE_mclust_11_annotate']\n",
    "pprint(STAGATE_annotate, sort_dicts=False)\n",
    "map_dict = {str(mclust): str(num+1) for num, (key, mclust_list) in enumerate(STAGATE_annotate.items()) for mclust in mclust_list}\n",
    "adata_STAGATE.obs['mclust_11_annotated'] = adata_STAGATE.obs['mclust_11'].astype(str).map(map_dict).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = 0.125\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "key = 'mclust_11_annotated'\n",
    "save_dir = output_path / f\"{key}\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# draw only the region of interest\n",
    "stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "for slice in range(1, 21):\n",
    "    slice_df = adata_STAGATE.obs.loc[adata_STAGATE.obs['slice'] == slice, [key]].astype(np.uint16)\n",
    "    stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(downsample, downsample))\n",
    "imwrite(save_dir / f\"{key}.tif\", stack_image, metadata={'axes': 'ZXY'})\n",
    "\n",
    "# save differnet gray scale in different channels\n",
    "for gray_scale in np.unique(stack_image):\n",
    "    if gray_scale == 0: continue\n",
    "    mask = stack_image == gray_scale\n",
    "    mask = mask.astype(np.uint16) * gray_scale\n",
    "    imwrite(save_dir / f\"{key}_{gray_scale}.tif\", mask, metadata={'axes': 'ZXY'})\n",
    "\n",
    "# combine all channels\n",
    "chn_files = sorted([str(save_dir/f) for f in os.listdir(save_dir) if re.search(r'_\\d+\\.tif$', f) and os.path.isfile(os.path.join(save_dir, f))],\n",
    "    key=lambda x: int(re.search(r'_(\\d+)\\.tif$', x).group(1)))\n",
    "ImarisFileConverter(chn_files, output_path=save_dir/f'{key}.ims')\n",
    "shutil.rmtree(save_dir / f\"{key}.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for folder in os.listdir(output_path):\n",
    "    if not 'leiden_subtype' in folder: continue\n",
    "    if os.path.isfile(os.path.join(output_path, folder)): continue\n",
    "    folder_path = os.path.join(output_path, folder)\n",
    "    num = folder.split('_')[1]\n",
    "    try: \n",
    "        shutil.move(os.path.join(folder_path, 'combined.ims'), os.path.join(output_path, f\"mclust_11_{num}_subtype.ims\"))\n",
    "        shutil.rmtree(folder_path)\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = 0.125\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "key = 'leiden_type'\n",
    "\n",
    "for clust in tqdm([7, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11], desc=\"STAGATE_mclust\"):\n",
    "    save_dir = output_path / f\"mclust_{clust}_{key}\"\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    # draw only the region of interest\n",
    "    adata_tmp = adata_STAGATE[adata_STAGATE.obs['mclust_11'] == str(clust)]\n",
    "    stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "    for slice in range(1, 21):\n",
    "        slice_df = adata_tmp.obs.loc[adata_tmp.obs['slice'] == slice, [key]].astype(np.uint16)\n",
    "        slice_df += 1\n",
    "        stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(downsample, downsample))\n",
    "    imwrite(save_dir / f\"{key}.tif\", stack_image, metadata={'axes': 'ZXY'})\n",
    "    # save differnet gray scale in different channels\n",
    "    for gray_scale in np.unique(stack_image):\n",
    "        if gray_scale == 0: continue\n",
    "        mask = stack_image == gray_scale\n",
    "        mask = mask.astype(np.uint16) * gray_scale\n",
    "        imwrite(save_dir / f\"{key}_{gray_scale}.tif\", mask, metadata={'axes': 'ZXY'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(analysis_dir, 'cell_typing_params.yaml'), 'r') as f:\n",
    "    annotaiton_params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "print(annotaiton_params.keys())\n",
    "print(list(annotaiton_params['leiden_annotation'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_STAGATE.obs.slice = adata_STAGATE.obs.slice.map(lambda x: int(x.split('slice')[1])+1)\n",
    "adata_STAGATE.obs.index = adata_STAGATE.obs.index.map(lambda x: x.split('-')[0])\n",
    "adata_STAGATE.obs.index = adata_STAGATE.obs.index + '-' + adata_STAGATE.obs.slice.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "downsample = 0.125\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "key = 'leiden_subtype'\n",
    "\n",
    "for clust in tqdm(list(range(1, 12)), desc=\"STAGATE_mclust\"):\n",
    "    save_dir = output_path / f\"mclust_{clust}_{key}\"\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    # draw only the region of interest\n",
    "    adata_tmp = adata_STAGATE[adata_STAGATE.obs['mclust_11'] == str(clust)]\n",
    "    stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "    for slice in range(1, 21):\n",
    "        slice_df = adata_tmp.obs.loc[adata_tmp.obs['slice'] == slice, [key]].astype(np.uint16)\n",
    "        slice_df += 1\n",
    "        stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(downsample, downsample))\n",
    "    imwrite(save_dir / f\"{key}.tif\", stack_image, metadata={'axes': 'ZXY'})\n",
    "    \n",
    "    # save differnet gray scale in different channels\n",
    "    for gray_scale in np.unique(stack_image):\n",
    "        if gray_scale == 0: continue\n",
    "        mask = stack_image == gray_scale\n",
    "        mask = mask.astype(np.uint16) * gray_scale\n",
    "        imwrite(save_dir / f\"{key}_{gray_scale}.tif\", mask, metadata={'axes': 'ZXY'})\n",
    "\n",
    "    # combine all channels\n",
    "    chn_files = sorted([str(save_dir/f) for f in os.listdir(save_dir) if re.search(r'_\\d+\\.tif$', f) and os.path.isfile(os.path.join(save_dir, f))],\n",
    "        key=lambda x: int(re.search(r'_(\\d+)\\.tif$', x).group(1)))\n",
    "    ImarisFileConverter(chn_files, output_path=output_path/f'mclust_{clust}_{key}.ims')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mclust_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 5000, 5625)\n"
     ]
    }
   ],
   "source": [
    "# save column key as different gray scale\n",
    "from tifffile import imwrite\n",
    "\n",
    "downsample = 0.125\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "\n",
    "key = 'mclust_10'\n",
    "save_dir = output_path / f\"{key}\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "for slice in range(1, 21):\n",
    "    slice_df = adata_STAGATE.obs.loc[adata_STAGATE.obs['slice'] == slice, [key]]\n",
    "    stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(downsample, downsample))\n",
    "# imwrite(save_dir / f\"{key}.tif\", stack_image, metadata={'axes': 'ZXY'})\n",
    "# stack_image = imread(save_dir / f\"{key}_downsample.tif\")\n",
    "print(stack_image.shape)\n",
    "for gray_scale in np.unique(stack_image):\n",
    "    if gray_scale == 0: continue\n",
    "    mask = stack_image == gray_scale\n",
    "    mask = mask.astype(np.uint16) * gray_scale\n",
    "    imwrite(save_dir / f\"{key}_{gray_scale}.tif\", mask, metadata={'axes': 'ZXY'})\n",
    "\n",
    "# combine all channels\n",
    "chn_files = sorted([str(save_dir/f) for f in os.listdir(save_dir) if re.search(r'_\\d+\\.tif$', f) and os.path.isfile(os.path.join(save_dir, f))],\n",
    "    key=lambda x: int(re.search(r'_(\\d+)\\.tif$', x).group(1)))\n",
    "ImarisFileConverter(chn_files, output_path=output_path/f'{key}.ims')\n",
    "shutil.rmtree(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tumor_AFP+': [1],\n",
      " 'Tumor_GPC3+': [3],\n",
      " 'Tumor_pro_CXCL13': [4],\n",
      " 'pioneer_immune': [5],\n",
      " 'CAF': [9],\n",
      " 'B': [8],\n",
      " 'Liver': [10],\n",
      " 'Epthelial_Neutrophil': [2, 6, 7]}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(analysis_dir / \"STAGATE_analysis_params.yaml\"), \"r\") as f:\n",
    "    STAGATE_params = yaml.load(f, Loader=yaml.FullLoader)\n",
    "STAGATE_annotate = STAGATE_params['STAGATE_mclust_10_annotate']\n",
    "pprint(STAGATE_annotate, sort_dicts=False)\n",
    "map_dict = {str(mclust): str(num+1) for num, (key, mclust_list) in enumerate(STAGATE_annotate.items()) for mclust in mclust_list}\n",
    "adata_STAGATE.obs['mclust_10_annotated'] = adata_STAGATE.obs['mclust_10'].astype(str).map(map_dict).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = 0.125\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "key = 'mclust_10_annotated'\n",
    "save_dir = output_path / f\"{key}\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# draw only the region of interest\n",
    "stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "for slice in range(1, 21):\n",
    "    slice_df = adata_STAGATE.obs.loc[adata_STAGATE.obs['slice'] == slice, [key]].astype(np.uint16)\n",
    "    stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(downsample, downsample))\n",
    "imwrite(save_dir / f\"{key}.tif\", stack_image, metadata={'axes': 'ZXY'})\n",
    "\n",
    "# save differnet gray scale in different channels\n",
    "for gray_scale in np.unique(stack_image):\n",
    "    if gray_scale == 0: continue\n",
    "    mask = stack_image == gray_scale\n",
    "    mask = mask.astype(np.uint16) * gray_scale\n",
    "    imwrite(save_dir / f\"{key}_{gray_scale}.tif\", mask, metadata={'axes': 'ZXY'})\n",
    "\n",
    "# combine all channels\n",
    "chn_files = sorted([str(save_dir/f) for f in os.listdir(save_dir) if re.search(r'_\\d+\\.tif$', f) and os.path.isfile(os.path.join(save_dir, f))],\n",
    "    key=lambda x: int(re.search(r'_(\\d+)\\.tif$', x).group(1)))\n",
    "ImarisFileConverter(chn_files, output_path=output_path/f'{key}.ims')\n",
    "shutil.rmtree(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGATE_mclust_10_type: 100%|██████████| 10/10 [53:42<00:00, 322.23s/it] \n"
     ]
    }
   ],
   "source": [
    "downsample = 0.125\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "key = 'leiden_type'\n",
    "\n",
    "for clust in tqdm(list(range(1, 11)), desc=\"STAGATE_mclust_10_type\"):\n",
    "    save_dir = output_path / f\"mclust_{clust}_{key}\"\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # draw only the region of interest\n",
    "    adata_tmp = adata_STAGATE[adata_STAGATE.obs['mclust_11'] == str(clust)]\n",
    "    stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "    for slice in range(1, 21):\n",
    "        slice_df = adata_tmp.obs.loc[adata_tmp.obs['slice'] == slice, [key]].astype(np.uint16)\n",
    "        slice_df += 1\n",
    "        stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(downsample, downsample))\n",
    "    \n",
    "    # save differnet gray scale in different channels\n",
    "    for gray_scale in np.unique(stack_image):\n",
    "        if gray_scale == 0: continue\n",
    "        mask = stack_image == gray_scale\n",
    "        mask = mask.astype(np.uint16) * gray_scale\n",
    "        imwrite(save_dir / f\"{key}_{gray_scale}.tif\", mask, metadata={'axes': 'ZXY'})\n",
    "    \n",
    "    # combine all channels\n",
    "    chn_files = sorted([str(save_dir/f) for f in os.listdir(save_dir) if re.search(r'_\\d+\\.tif$', f) and os.path.isfile(os.path.join(save_dir, f))],\n",
    "        key=lambda x: int(re.search(r'_(\\d+)\\.tif$', x).group(1)))\n",
    "    ImarisFileConverter(chn_files, output_path=output_path/f'mclust_10_{clust}_{key}.ims')\n",
    "    shutil.rmtree(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\tother\n",
      "1\tLiver\n",
      "2\tTumor\n",
      "3\tEndo\n",
      "4\tEp\n",
      "5\tCAF\n",
      "6\tDC\n",
      "7\tMait\n",
      "8\tMast\n",
      "9\tMonocyte\n",
      "10\tNeutrophil\n",
      "11\tMacrophage\n",
      "12\tCD4+\n",
      "13\tCD8+\n",
      "14\tT_reg\n",
      "15\tB\n",
      "16\tNK\n"
     ]
    }
   ],
   "source": [
    "# leiden_subtyp列e和subtype列是一一对应的，打印leiden_subtyp列e对应的subtype列的值\n",
    "for i in sorted([int(_) for _ in adata_STAGATE.obs['leiden_type'].unique()]):\n",
    "    name = list(adata_STAGATE.obs.loc[adata_STAGATE.obs['leiden_type'] == str(i), 'type'].unique())[0]\n",
    "    print(f\"{i+1}\\t{name}\")\n",
    "    # print(f\"\\'{name}\\'\", end=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGATE_mclust_10_subtype: 100%|██████████| 10/10 [2:15:30<00:00, 813.08s/it] \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "downsample = 0.125\n",
    "image_shape = project_params['image_shape'][downsample]\n",
    "key = 'leiden_subtype'\n",
    "\n",
    "for clust in tqdm(list(range(1, 11)), desc=\"STAGATE_mclust_10_subtype\"):\n",
    "    save_dir = output_path / f\"mclust_{clust}_{key}\"\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # draw only the region of interest\n",
    "    adata_tmp = adata_STAGATE[adata_STAGATE.obs['mclust_10'] == str(clust)]\n",
    "    stack_image = np.zeros(shape=(20, image_shape[0], image_shape[1]), dtype=np.uint16)\n",
    "    for slice in range(1, 21):\n",
    "        slice_df = adata_tmp.obs.loc[adata_tmp.obs['slice'] == slice, [key]].astype(np.uint16)\n",
    "        slice_df += 1\n",
    "        stack_image[slice-1,:,:] = color_cells_uint16(image_shape, hulls_all, slice_df, key, resize=(downsample, downsample))\n",
    "    \n",
    "    # save differnet gray scale in different channels\n",
    "    for gray_scale in np.unique(stack_image):\n",
    "        if gray_scale == 0: continue\n",
    "        mask = stack_image == gray_scale\n",
    "        mask = mask.astype(np.uint16) * gray_scale\n",
    "        imwrite(save_dir / f\"{key}_{gray_scale}.tif\", mask, metadata={'axes': 'ZXY'})\n",
    "\n",
    "    # combine all channels\n",
    "    chn_files = sorted([str(save_dir/f) for f in os.listdir(save_dir) if re.search(r'_\\d+\\.tif$', f) and os.path.isfile(os.path.join(save_dir, f))],\n",
    "        key=lambda x: int(re.search(r'_(\\d+)\\.tif$', x).group(1)))\n",
    "    ImarisFileConverter(chn_files, output_path=output_path/f'mclust_10_{clust}_{key}.ims')\n",
    "    shutil.rmtree(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\tother\n",
      "1\tLiver\n",
      "2\tTumor_AFP+\n",
      "3\tTumor_GPC3+\n",
      "4\tTumor_proliferation\n",
      "5\tEndo_PECAM1+\n",
      "6\tEp_EPCAM+\n",
      "7\tCAF_ACTA2+\n",
      "8\tcDC1_CLEC9A+\n",
      "9\tcDC2_CD1C+\n",
      "10\tpDC_LILRA4+\n",
      "11\tMait_SLC4A10+\n",
      "12\tMast_CPA3+\n",
      "13\tMonocyte_CD14+\n",
      "14\tMonocyte_CD14+, CD16+\n",
      "15\tMonocyte_CD16+\n",
      "16\tNeutrophil_CSF3R+\n",
      "17\tNeutrophil_CSF3R+, S100A8+\n",
      "18\tMacrophage_LYVE1+\n",
      "19\tCyto_T_CD4+\n",
      "20\tT_CD4+, CTLA4+\n",
      "21\tT_CD4+, CXCL13+\n",
      "22\tT_CD4+, PD1+\n",
      "23\tT_CD4+, PD1+, CTLA4+\n",
      "24\tCyto_T_CD8+\n",
      "25\tT_CD8+, CXCL13+\n",
      "26\tT_CD8+, PD1+\n",
      "27\tT_reg\n",
      "28\tB_CD79A+\n",
      "29\tB_CD79A+, MS4A1+\n",
      "30\tB_MS4A1+\n",
      "31\tPlasma_B_CD79A+, MZB1+\n",
      "32\tNK_NCAM1+\n"
     ]
    }
   ],
   "source": [
    "# leiden_subtyp列e和subtype列是一一对应的，打印leiden_subtyp列e对应的subtype列的值\n",
    "for i in sorted([int(_) for _ in adata_STAGATE.obs['leiden_subtype'].unique()]):\n",
    "    name = list(adata_STAGATE.obs.loc[adata_STAGATE.obs['leiden_subtype'] == str(i), 'subtype'].unique())[0]\n",
    "    print(f\"{i+1}\\t{name}\")\n",
    "    # print(f\"\\'{name}\\'\", end=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRISM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
